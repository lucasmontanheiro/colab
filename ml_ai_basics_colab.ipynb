{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f82627e",
   "metadata": {},
   "source": [
    "\n",
    "# ML & AI Basics â€” Handsâ€‘On Colab Crash Course\n",
    "\n",
    "Welcome! This notebook is designed to teach you **core machine learning concepts** by doing.  \n",
    "You'll run cells, tweak parameters, and see how results change â€” all inside **Google Colab**.\n",
    "\n",
    "**What you'll learn:**\n",
    "- The ML workflow: data â†’ split â†’ train â†’ evaluate â†’ iterate\n",
    "- Classic ML with **scikit-learn** (Iris dataset)\n",
    "- Model evaluation (accuracy, precision/recall, confusion matrix)\n",
    "- Hyperparameter tuning with cross-validation\n",
    "- Intro to **neural networks** with **Keras/TensorFlow** (MNIST digits)\n",
    "- (Optional) A tiny taste of **NLP** with Hugging Face Transformers\n",
    "\n",
    "> Tip: In Colab, use `Runtime â†’ Run all` to execute the whole notebook, or run cells one by one with **Shift+Enter**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05293f91",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup\n",
    "This notebook uses common Python libraries. The cells below will import what we need and (optionally) install extras if you're running in a fresh environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a461c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you're in Colab, most of these are already installed. If anything is missing, uncomment to install.\n",
    "# !pip -q install -U scikit-learn pandas matplotlib\n",
    "# Optional (only for the NLP section near the end):\n",
    "# !pip -q install -U transformers torch --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import sys, platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}  |  Platform: {platform.platform()}\" )\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21991d24",
   "metadata": {},
   "source": [
    "\n",
    "## Part A â€” Classic ML with scikitâ€‘learn (Iris)\n",
    "We'll walk through the **endâ€‘toâ€‘end ML workflow** using the classic Iris dataset.\n",
    "\n",
    "**Concepts covered:**\n",
    "1. Load and inspect data (features vs. labels)\n",
    "2. Train/test split\n",
    "3. Train a baseline model (Logistic Regression)\n",
    "4. Evaluate (accuracy, precision/recall, confusion matrix)\n",
    "5. Crossâ€‘validation\n",
    "6. Hyperparameter tuning with `GridSearchCV`\n",
    "7. Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cc08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Load & inspect the dataset\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "df = iris.frame.copy()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic info & class balance\n",
    "display(df.describe(include='all'))\n",
    "print(\"\\nClass distribution (target):\\n\", df['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2091693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Quick EDA plots\n",
    "# Plot class distribution\n",
    "df['target'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Class Distribution (Iris target)')\n",
    "plt.xlabel('Class ID (0=setosa,1=versicolor,2=virginica)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scatter matrix to visualize feature relationships\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(df[iris.feature_names], figsize=(8,8))\n",
    "plt.suptitle('Feature Scatter Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a57056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Split data\n",
    "X = df[iris.feature_names]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ed3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Build a baseline model (with scaling in a pipeline)\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Confusion matrix plot\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
    "disp.plot(values_format='d')\n",
    "plt.title('Confusion Matrix â€” Logistic Regression (Iris)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Cross-validation on the training set\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Crossâ€‘val scores:\", cv_scores)\n",
    "print(\"Mean Â± std:\", np.mean(cv_scores), \"+/âˆ’\", np.std(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7) Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'clf__C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV score:\", grid.best_score_)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "test_acc = best_model.score(X_test, y_test)\n",
    "print(\"Test accuracy (best model):\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fba30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8) Save the trained model (so you can reâ€‘use it later)\n",
    "import joblib, os\n",
    "\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "model_path = 'artifacts/iris_logreg_pipeline.joblib'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Saved: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f50ea",
   "metadata": {},
   "source": [
    "\n",
    "### âœ… Miniâ€‘Exercises (try these right in the notebook)\n",
    "1. Change `test_size` to `0.3` â€” what happens to accuracy?\n",
    "2. Replace `LogisticRegression` with `RandomForestClassifier` â€” which performs better?\n",
    "3. Remove `StandardScaler` in the pipeline â€” how does that impact results?\n",
    "4. Add `scoring='f1_macro'` in crossâ€‘validation â€” how do conclusions change vs. accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff695be3",
   "metadata": {},
   "source": [
    "\n",
    "## Part B â€” Intro to Neural Networks with Keras/TensorFlow (MNIST)\n",
    "We'll train a small fullyâ€‘connected neural network on **MNIST** handwritten digits.  \n",
    "This introduces concepts like **epochs**, **batches**, and **layers**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae70226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If TensorFlow is missing in your environment, uncomment this in Colab:\n",
    "# !pip -q install -U tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    print(\"TensorFlow:\", tf.__version__)\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow not available. If you're in Colab, run the install cell above.\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b976d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Load data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten 28x28 images to vectors of length 784\n",
    "x_train = x_train.reshape((-1, 28*28))\n",
    "x_test  = x_test.reshape((-1, 28*28))\n",
    "\n",
    "print(\"Train:\", x_train.shape, y_train.shape, \" Test:\", x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e4201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Build a simple model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Train\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Evaluate\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e53fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Plot training curves\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ffa52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Predict a few samples\n",
    "preds = model.predict(x_test[:10])\n",
    "pred_labels = np.argmax(preds, axis=1)\n",
    "print(\"Predicted labels:\", pred_labels)\n",
    "print(\"True labels:     \", y_test[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c20f21",
   "metadata": {},
   "source": [
    "\n",
    "### âœ… Miniâ€‘Exercises\n",
    "1. Increase `epochs` from 5 â†’ 10 â€” does accuracy improve?\n",
    "2. Try a different architecture (e.g., add `Dropout(0.2)` layers) â€” any change?\n",
    "3. Change `batch_size` (64, 256) â€” what happens? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ace56",
   "metadata": {},
   "source": [
    "\n",
    "## (Optional) Part C â€” Tiny Taste of NLP with Transformers\n",
    "This section shows a preâ€‘trained text classifier. It uses **Hugging Face Transformers** to run inference with a small sentiment model.\n",
    "\n",
    "> This may download a small model the first time it runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed in Colab, install first:\n",
    "# !pip -q install -U transformers torch --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    nlp = pipeline('sentiment-analysis')\n",
    "    print(nlp([\"I love learning ML!\", \"This model seems slow.\"]))\n",
    "except Exception as e:\n",
    "    print(\"Transformers not available. If you're in Colab, run the install cell above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863fb2b9",
   "metadata": {},
   "source": [
    "\n",
    "## Wrapâ€‘Up & Next Steps\n",
    "You just went through:\n",
    "- A complete **classic ML** pipeline with scikitâ€‘learn\n",
    "- A simple **neural network** with Keras/TensorFlow\n",
    "- A tiny **NLP** demo with Transformers\n",
    "\n",
    "**Where to go from here:**\n",
    "- Try new datasets (e.g., `datasets.load_wine()`, `load_breast_cancer()`)\n",
    "- Swap models: SVMs, Random Forests, XGBoost (need `xgboost` install)\n",
    "- Build a simple **Flask** or **FastAPI** service to serve your model (then deploy to Cloud Run)\n",
    "- Explore **colab notebooks** from TensorFlow tutorials and Kaggle\n",
    "\n",
    "Happy learning! ðŸš€\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
