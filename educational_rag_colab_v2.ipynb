{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33896447",
   "metadata": {},
   "source": [
    "# 🧪 Educational RAG in Google Colab (OpenAI SDK v1.x compatible)\n",
    "Minimal Retrieval-Augmented Generation pipeline using free components for indexing and the OpenAI API for generation.\n",
    "\n",
    "**What this does:**\n",
    "- Index your PDFs or .txt files\n",
    "- Create embeddings with `sentence-transformers`\n",
    "- Store vectors in FAISS\n",
    "- Retrieve top-k chunks\n",
    "- Generate an answer with the **OpenAI Python SDK v1.x** (`openai` package)\n",
    "\n",
    "---\n",
    "## How to use\n",
    "1) Run **Install** ➜ 2) **Setup & Helpers** ➜ 3) **Load documents** ➜ 4) **Index** ➜ 5) **Ask**\n",
    "If you don't have files yet, there's an inline `sample_text` fallback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45052db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install faiss-cpu sentence-transformers pypdf openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9267103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "\n",
    "# ---- OpenAI client (v1.x) ----\n",
    "# Set your API key in the Colab environment: Runtime > Run again after setting\n",
    "# or do: os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY', None))\n",
    "if client.api_key is None:\n",
    "    print('⚠️ OPENAI_API_KEY is not set. Set it with `os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"` or in Colab > Secrets.')\n",
    "\n",
    "# ---- Chunker ----\n",
    "def chunk_text(text: str, chunk_size: int = 500, chunk_overlap: int = 100):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(len(words), start + chunk_size)\n",
    "        chunks.append(' '.join(words[start:end]))\n",
    "        start = end - chunk_overlap if end - chunk_overlap > start else end\n",
    "    return chunks\n",
    "\n",
    "# ---- Prompt template ----\n",
    "def build_prompt(question: str, context: str) -> str:\n",
    "    sys = 'Use ONLY the provided context to answer. If the answer is not in the context, say you do not know.'\n",
    "    return f\"{sys}\\n\\nQuestion: {question}\\n\\nContext:\\n{context}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📄 1) Load your documents (PDF or .txt). Add absolute paths below after uploading to Colab Files.\n",
    "doc_paths = []  # e.g., ['/content/your.pdf', '/content/notes.txt']\n",
    "\n",
    "all_chunks: List[str] = []\n",
    "chunk_meta: List[Dict] = []\n",
    "\n",
    "for path in doc_paths:\n",
    "    text = ''\n",
    "    if path.lower().endswith('.pdf'):\n",
    "        reader = PdfReader(path)\n",
    "        text = '\\n'.join([(p.extract_text() or '') for p in reader.pages])\n",
    "    else:\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "    chunks = chunk_text(text)\n",
    "    all_chunks.extend(chunks)\n",
    "    chunk_meta.extend([{'source': path}] * len(chunks))\n",
    "\n",
    "# Fallback: inline sample text so the notebook works even with no files\n",
    "if not all_chunks:\n",
    "    sample_text = (\n",
    "        'Retrieval-augmented generation (RAG) combines a retriever and a generator. '\n",
    "        'Embeddings turn text into vectors. FAISS enables efficient nearest-neighbor search. '\n",
    "        'Chunking controls how text is split before indexing.'\n",
    "    )\n",
    "    all_chunks = chunk_text(sample_text)\n",
    "    chunk_meta = [{'source': 'inline_sample'} for _ in all_chunks]\n",
    "\n",
    "print(f'Loaded {len(all_chunks)} chunks from {len(doc_paths)} documents (or inline sample).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f603140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 2) Build embeddings & FAISS index (robust)\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "X = emb_model.encode(all_chunks, normalize_embeddings=True)\n",
    "X = np.atleast_2d(X).astype('float32')  # ensure 2D float32\n",
    "\n",
    "if X.size == 0:\n",
    "    raise ValueError('No embeddings to index. Ensure documents or sample_text are present.')\n",
    "\n",
    "d = X.shape[1]\n",
    "index = faiss.IndexFlatIP(d)  # cosine via inner-product on normalized vectors\n",
    "index.add(X)\n",
    "print('FAISS index size:', index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3409f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5):\n",
    "    if index.ntotal == 0:\n",
    "        raise ValueError('The FAISS index is empty. Run the indexing cell after loading documents.')\n",
    "    qv = emb_model.encode([query], normalize_embeddings=True).astype('float32')\n",
    "    D, I = index.search(qv, min(top_k, index.ntotal))\n",
    "    results = [(float(D[0][j]), all_chunks[i], chunk_meta[i]) for j, i in enumerate(I[0]) if i != -1]\n",
    "    return results\n",
    "\n",
    "# Quick test\n",
    "retrieve('What is RAG?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a1261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(question: str, k: int = 5, model: str = 'gpt-4o-mini') -> str:\n",
    "    results = retrieve(question, top_k=k)\n",
    "    if not results:\n",
    "        return 'I could not retrieve any context. Please add documents first.'\n",
    "    context = '\\n\\n---\\n\\n'.join([r[1] for r in results])\n",
    "    prompt = build_prompt(question, context)\n",
    "    if client.api_key is None:\n",
    "        return 'OPENAI_API_KEY not set. Please set it before calling the model.'\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf6495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💡 Try your own question here\n",
    "q = 'Explain how chunk size affects retrieval.'\n",
    "print(rag_answer(q))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
