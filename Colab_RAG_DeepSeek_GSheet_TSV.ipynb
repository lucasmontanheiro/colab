{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e27254",
   "metadata": {},
   "source": [
    "\n",
    "# RAG over a Google Sheet TSV (with DeepSeek API) — Colab Notebook\n",
    "\n",
    "This notebook lets you run **Retrieval-Augmented Generation (RAG)** over a **Google Sheet exported as TSV** (or any TSV URL) and use **DeepSeek** for the generation step.\n",
    "\n",
    "**What you get:**\n",
    "- No need for Google Sheets API — just a TSV link.\n",
    "- Local, zero-cost embeddings (SentenceTransformers) + FAISS index.\n",
    "- Controls for `k` (how many results you retrieve), chunking, max context size, temperature, etc.\n",
    "- Clear, educational notes throughout so you can learn and tweak.\n",
    "\n",
    "> **Tip:** In Google Sheets: File → Share → Publish to the web → Tab-separated values (TSV) link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba352969",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Setup\n",
    "\n",
    "We install the minimal dependencies:\n",
    "- `pandas` for loading TSV.\n",
    "- `sentence-transformers` for local embeddings (free).\n",
    "- `faiss-cpu` as a fast vector index.\n",
    "- `ipywidgets` for interactive controls.\n",
    "- `requests` to call DeepSeek's API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally or on Colab, uncomment as needed.\n",
    "!pip -q install pandas sentence-transformers faiss-cpu ipywidgets requests\n",
    "# Enable widgets in Colab\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b890579",
   "metadata": {},
   "source": [
    "\n",
    "## 2) DeepSeek API key & config\n",
    "\n",
    "- We'll call the **DeepSeek Chat Completions** endpoint.\n",
    "- Enter your API key below (it won't be printed).\n",
    "- You can tweak model, temperature, and max tokens later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import getpass\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Enter your DeepSeek API key here (recommended: use a secrets manager when possible)\n",
    "deepseek_api_key = widgets.Password(\n",
    "    description='API Key:',\n",
    "    placeholder='sk-...',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Advanced: base URL & model (defaults should work)\n",
    "deepseek_base_url = widgets.Text(\n",
    "    description='Base URL:',\n",
    "    value='https://api.deepseek.com',\n",
    "    layout=widgets.Layout(width='70%')\n",
    ")\n",
    "\n",
    "deepseek_model = widgets.Text(\n",
    "    description='Model:',\n",
    "    value='deepseek-chat',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "display(deepseek_api_key, deepseek_base_url, deepseek_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd77cf8",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Load your TSV\n",
    "\n",
    "Paste a link to a **TSV**. This can be a Google Sheet published as TSV or any TSV hosted online.\n",
    "\n",
    "**Notes:**\n",
    "- No Google Sheets API needed.\n",
    "- If your sheet contains headers in the first row, we will use them as column names.\n",
    "- You can choose which columns to index (for retrieval) and which to display as sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d07635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "tsv_url = widgets.Text(\n",
    "    description='TSV URL:',\n",
    "    placeholder='https://.../your.tsv',\n",
    "    layout=widgets.Layout(width='90%')\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(description='Load TSV', button_style='primary', icon='download')\n",
    "\n",
    "out_load = widgets.Output()\n",
    "\n",
    "def on_load_clicked(b):\n",
    "    out_load.clear_output()\n",
    "    with out_load:\n",
    "        if not tsv_url.value.strip():\n",
    "            print(\"Please provide a TSV URL.\")\n",
    "            return\n",
    "        try:\n",
    "            df = pd.read_csv(tsv_url.value.strip(), sep='\\t', dtype=str, keep_default_na=False, on_bad_lines='skip')\n",
    "            print(f\"Loaded {len(df)} rows, {len(df.columns)} columns.\")\n",
    "            display(df.head(3))\n",
    "            # store globally\n",
    "            globals()['_rag_df'] = df\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load TSV:\", e)\n",
    "\n",
    "load_button.on_click(on_load_clicked)\n",
    "\n",
    "display(tsv_url, load_button, out_load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7c3fa",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Choose columns & chunking strategy\n",
    "\n",
    "**What gets embedded?**  \n",
    "The RAG index needs a **text representation** of each row. We typically join multiple columns into one chunk string.  \n",
    "\n",
    "- **Columns to index**: We’ll concatenate these to form the text we embed and search over.  \n",
    "- **Columns to show**: These will be presented with the answer as “sources.”  \n",
    "\n",
    "**Chunking**: For spreadsheets, a good default is **one row = one chunk**. If your rows are very long, you can add a **max characters per chunk** to avoid huge context. We also include a **max context tokens** control to keep prompts small and cheap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3bd063",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def get_df():\n",
    "    df = globals().get('_rag_df', None)\n",
    "    if df is None:\n",
    "        raise RuntimeError(\"No TSV loaded yet. Go back to step 3.\")\n",
    "    return df\n",
    "\n",
    "def make_multiselect(options, description):\n",
    "    return widgets.SelectMultiple(\n",
    "        options=options,\n",
    "        description=description,\n",
    "        layout=widgets.Layout(width='90%', height='150px')\n",
    "    )\n",
    "\n",
    "setup_button = widgets.Button(description='Configure Columns', icon='cog')\n",
    "out_setup = widgets.Output()\n",
    "\n",
    "index_cols_widget = None\n",
    "show_cols_widget = None\n",
    "id_col_widget = None\n",
    "\n",
    "max_chars_per_chunk = widgets.IntSlider(description=\"Max chars/chunk\", value=1000, min=200, max=5000, step=100)\n",
    "max_context_tokens = widgets.IntSlider(description=\"Max context tokens\", value=1200, min=200, max=6000, step=100)\n",
    "\n",
    "def on_setup_clicked(b):\n",
    "    global index_cols_widget, show_cols_widget, id_col_widget\n",
    "    out_setup.clear_output()\n",
    "    with out_setup:\n",
    "        df = get_df()\n",
    "        columns = list(df.columns)\n",
    "        index_cols_widget = make_multiselect(columns, \"Index cols\")\n",
    "        show_cols_widget = make_multiselect(columns, \"Show cols\")\n",
    "        id_col_widget = widgets.Dropdown(options=['<row_number>'] + columns, description='ID column')\n",
    "        display(index_cols_widget, show_cols_widget, id_col_widget, max_chars_per_chunk, max_context_tokens)\n",
    "\n",
    "display(setup_button, out_setup)\n",
    "setup_button.on_click(on_setup_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc007d",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Build embeddings & FAISS index\n",
    "\n",
    "We use `sentence-transformers/all-MiniLM-L6-v2` (free, fast, solid baseline).  \n",
    "We **normalize vectors** and use **inner product (cosine)** in FAISS.\n",
    "\n",
    "> Educational note: **k (top-k)** controls how many chunks to fetch before building the prompt. Too high → expensive & noisy. Too low → risk missing relevant context. Start with **k=3–5**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f70393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "build_button = widgets.Button(description='Build Index', button_style='success', icon='hammer')\n",
    "out_build = widgets.Output()\n",
    "\n",
    "# Controls for retrieval\n",
    "k_slider = widgets.IntSlider(description='k (top results)', value=4, min=1, max=15, step=1)\n",
    "min_sim_threshold = widgets.FloatSlider(description='Min similarity', value=0.2, min=0.0, max=0.99, step=0.01)\n",
    "\n",
    "def row_to_text(row, cols):\n",
    "    # Join selected columns for the chunk text\n",
    "    parts = []\n",
    "    for c in cols:\n",
    "        parts.append(f\"{c}: {row.get(c, '')}\")\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "def trim_text(s, max_chars):\n",
    "    return s if len(s) <= max_chars else s[:max_chars] + \" …\"\n",
    "\n",
    "def on_build_clicked(b):\n",
    "    out_build.clear_output()\n",
    "    with out_build:\n",
    "        try:\n",
    "            df = get_df()\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "            return\n",
    "\n",
    "        if k_slider.value < 1:\n",
    "            print(\"k must be >= 1\")\n",
    "            return\n",
    "\n",
    "        if globals().get('index_cols_widget', None) is None or len(index_cols_widget.value) == 0:\n",
    "            print(\"Please choose at least one column to index (Step 4).\")\n",
    "            return\n",
    "\n",
    "        idx_cols = list(index_cols_widget.value)\n",
    "        show_cols = list(show_cols_widget.value) if show_cols_widget and len(show_cols_widget.value)>0 else idx_cols\n",
    "        use_id_col = id_col_widget.value if id_col_widget else '<row_number>'\n",
    "\n",
    "        # Build documents (one row = one chunk)\n",
    "        docs = []\n",
    "        meta = []  # store row id + raw fields to show later\n",
    "        for i, row in get_df().iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            chunk_text = trim_text(row_to_text(row_dict, idx_cols), max_chars_per_chunk.value)\n",
    "            row_id = row_dict.get(use_id_col, i) if use_id_col != '<row_number>' else i\n",
    "            docs.append(chunk_text)\n",
    "            meta.append({\n",
    "                'row_index': i,\n",
    "                'row_id': row_id,\n",
    "                'show': {c: row_dict.get(c, '') for c in show_cols}\n",
    "            })\n",
    "\n",
    "        print(f\"Building embeddings for {len(docs)} chunks...\")\n",
    "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        vecs = model.encode(docs, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "        dim = vecs.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)  # inner product; with normalized vectors, equals cosine sim\n",
    "        index.add(vecs)\n",
    "\n",
    "        globals()['_rag_index'] = index\n",
    "        globals()['_rag_vecs'] = vecs\n",
    "        globals()['_rag_docs'] = docs\n",
    "        globals()['_rag_meta'] = meta\n",
    "        globals()['_rag_model'] = model\n",
    "        globals()['_rag_setup'] = dict(idx_cols=idx_cols, show_cols=show_cols, use_id_col=use_id_col)\n",
    "\n",
    "        print(\"Index built.\")\n",
    "        print(\"— Dimensions:\", dim)\n",
    "        print(\"— Chunks:\", len(docs))\n",
    "        print(\"— Example chunk:\", docs[0][:200])\n",
    "\n",
    "display(k_slider, min_sim_threshold, build_button, out_build)\n",
    "build_button.on_click(on_build_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdaef85",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Ask questions (RAG) and generate with DeepSeek\n",
    "\n",
    "**How it works:**\n",
    "1. We embed your query locally.\n",
    "2. We retrieve top-`k` results from FAISS (filtering by min similarity if set).\n",
    "3. We **truncate the concatenated context** to keep within your **max context tokens** (approximate).\n",
    "4. We build a prompt that tells the model to answer **using only the provided context**.\n",
    "5. We call **DeepSeek** for the final answer.\n",
    "\n",
    "You can tweak:\n",
    "- `k (top results)`\n",
    "- `min similarity`\n",
    "- `max context tokens`\n",
    "- `temperature` and `max tokens` for generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb7d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "query_box = widgets.Textarea(\n",
    "    description='Question:',\n",
    "    placeholder='Ask a question about your TSV data...',\n",
    "    layout=widgets.Layout(width='90%', height='80px')\n",
    ")\n",
    "\n",
    "temperature = widgets.FloatSlider(description='Temperature', value=0.2, min=0.0, max=1.5, step=0.05)\n",
    "gen_max_tokens = widgets.IntSlider(description='Gen max tokens', value=512, min=32, max=4096, step=32)\n",
    "\n",
    "run_button = widgets.Button(description='Run RAG', button_style='primary', icon='play')\n",
    "out_run = widgets.Output()\n",
    "\n",
    "def approx_token_len(text):\n",
    "    # crude approx: 1 token ~ 4 chars (varies by tokenizer/model)\n",
    "    return max(1, math.ceil(len(text) / 4))\n",
    "\n",
    "def build_context(query_vec, k, min_sim):\n",
    "    index = globals().get('_rag_index', None)\n",
    "    vecs = globals().get('_rag_vecs', None)\n",
    "    docs = globals().get('_rag_docs', None)\n",
    "    meta = globals().get('_rag_meta', None)\n",
    "    model = globals().get('_rag_model', None)\n",
    "\n",
    "    if index is None:\n",
    "        raise RuntimeError(\"No index built yet. Run Step 5.\")\n",
    "    if model is None:\n",
    "        raise RuntimeError(\"Embedding model not found. Rebuild index.\")\n",
    "\n",
    "    # encode query\n",
    "    qv = model.encode([query_box.value], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    # search\n",
    "    D, I = index.search(qv, min(k, index.ntotal))\n",
    "    I = I[0]; D = D[0]\n",
    "\n",
    "    # filter by similarity threshold\n",
    "    pairs = [(int(i), float(d)) for i, d in zip(I, D) if float(d) >= min_sim]\n",
    "    if not pairs:\n",
    "        return \"\", []\n",
    "\n",
    "    # take top-k after filtering\n",
    "    top = pairs[:k]\n",
    "    # format context blocks with provenance\n",
    "    blocks = []\n",
    "    sources = []\n",
    "    for idx, sim in top:\n",
    "        md = meta[idx]\n",
    "        show_kv = \" | \".join(f\"{k}: {v}\" for k, v in md['show'].items())\n",
    "        blocks.append(f\"[row_id={md['row_id']}; sim={sim:.3f}] {docs[idx]}\")\n",
    "        sources.append({'row_id': md['row_id'], 'similarity': sim, **md['show']})\n",
    "\n",
    "    context = \"\\n\\n\".join(blocks)\n",
    "    return context, sources\n",
    "\n",
    "def truncate_context_by_tokens(context, max_tokens):\n",
    "    # very rough truncation based on char length; keeps head\n",
    "    approx = approx_token_len(context)\n",
    "    if approx <= max_tokens:\n",
    "        return context\n",
    "    # truncate head\n",
    "    target_chars = max_tokens * 4\n",
    "    return context[:target_chars] + \"\\n\\n… (truncated)\"\n",
    "\n",
    "def call_deepseek_chat(system_prompt, user_prompt):\n",
    "    base = deepseek_base_url.value.rstrip('/')\n",
    "    url = f\"{base}/chat/completions\" if not base.endswith(\"/v1\") else f\"{base}/chat/completions\"\n",
    "    # If your endpoint requires /v1/chat/completions, set base to https://api.deepseek.com/v1\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {deepseek_api_key.value.strip()}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": deepseek_model.value.strip() or \"deepseek-chat\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"temperature\": float(temperature.value),\n",
    "        \"max_tokens\": int(gen_max_tokens.value),\n",
    "        \"stream\": False\n",
    "    }\n",
    "    resp = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    # OpenAI-compatible shape\n",
    "    return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def format_sources_table(sources):\n",
    "    if not sources:\n",
    "        return \"No sources.\"\n",
    "    # simple Markdown table\n",
    "    headers = list(sources[0].keys())\n",
    "    lines = [\"|\" + \"|\".join(headers) + \"|\", \"|\" + \"|\".join([\"---\"]*len(headers)) + \"|\"]\n",
    "    for s in sources:\n",
    "        row = [str(s.get(h, \"\")) for h in headers]\n",
    "        lines.append(\"|\" + \"|\".join(row) + \"|\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    out_run.clear_output()\n",
    "    with out_run:\n",
    "        if not deepseek_api_key.value.strip():\n",
    "            print(\"Please enter your DeepSeek API key in Step 2.\")\n",
    "            return\n",
    "        if not query_box.value.strip():\n",
    "            print(\"Please enter a question.\")\n",
    "            return\n",
    "        try:\n",
    "            ctx, sources = build_context(\n",
    "                query_vec=None,\n",
    "                k=int(k_slider.value),\n",
    "                min_sim=float(min_sim_threshold.value)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Error during retrieval:\", e)\n",
    "            return\n",
    "\n",
    "        if not ctx.strip():\n",
    "            print(\"No matching context found above your similarity threshold. Try lowering the threshold or increasing k.\")\n",
    "            return\n",
    "\n",
    "        ctx_trunc = truncate_context_by_tokens(ctx, int(max_context_tokens.value))\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a careful assistant. Answer the user's question using ONLY the provided context. \"\n",
    "            \"If the answer isn't contained in the context, say you don't know.\"\n",
    "        )\n",
    "        user_prompt = f\"Context:\\n{ctx_trunc}\\n\\nQuestion:\\n{query_box.value}\\n\\nAnswer concisely:\"\n",
    "\n",
    "        try:\n",
    "            answer = call_deepseek_chat(system_prompt, user_prompt)\n",
    "        except requests.HTTPError as e:\n",
    "            print(\"DeepSeek API error:\", e.response.text[:500])\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(\"Failed calling DeepSeek API:\", e)\n",
    "            return\n",
    "\n",
    "        display(Markdown(\"### Answer\"))\n",
    "        display(Markdown(answer))\n",
    "\n",
    "        display(Markdown(\"### Sources (top-k by similarity)\"))\n",
    "        display(Markdown(format_sources_table(sources)))\n",
    "\n",
    "display(query_box, temperature, gen_max_tokens, run_button, out_run)\n",
    "run_button.on_click(on_run_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2039dc6",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Educational notes — the important knobs\n",
    "\n",
    "- **k (top results):** How many chunks you retrieve. Start at **3–5**. Higher k increases cost and can add noise; lower k risks missing relevant info.  \n",
    "- **Min similarity:** Filter out low-quality matches. Raise it (e.g., 0.3–0.5) to be stricter; lower it if you’re missing answers.  \n",
    "- **Max chars per chunk:** Avoids creating huge chunks from very long rows. Helps keep prompts small.  \n",
    "- **Max context tokens:** Hard cap for how much context you pass to the model; keeps cost/latency predictable.  \n",
    "- **Temperature:** Lower is more deterministic; higher can be more creative but less faithful to context.  \n",
    "- **Prompting style:** We instruct the model to answer **only** from context. This reduces hallucinations.\n",
    "\n",
    "**When to consider re-ranking:**  \n",
    "If your dataset is big or noisy, consider a two-stage approach: retrieve k=10, then re-rank down to 3 using a local cross-encoder (e.g., `cross-encoder/ms-marco-MiniLM-L-6-v2`). This adds compute but can boost precision.\n",
    "\n",
    "**Scaling tips:**  \n",
    "- Cache embeddings and the FAISS index to disk.  \n",
    "- Only re-embed changed rows by hashing row contents.  \n",
    "- For *very* large sheets, move to a persistent vector store (e.g., Qdrant) but keep the same interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33e373",
   "metadata": {},
   "source": [
    "\n",
    "## 8) (Optional) Save / Load the index\n",
    "\n",
    "Use these helpers to persist your work between sessions. You’ll still need to re-load the TSV to reconstruct metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d333f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle, os, faiss\n",
    "\n",
    "save_prefix = 'rag_index'\n",
    "\n",
    "def save_index(prefix=save_prefix):\n",
    "    index = globals().get('_rag_index', None)\n",
    "    vecs = globals().get('_rag_vecs', None)\n",
    "    docs = globals().get('_rag_docs', None)\n",
    "    meta = globals().get('_rag_meta', None)\n",
    "    setup = globals().get('_rag_setup', None)\n",
    "    if index is None:\n",
    "        print(\"No index to save.\")\n",
    "        return\n",
    "    faiss.write_index(index, f\"{prefix}.faiss\")\n",
    "    with open(f\"{prefix}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(vecs=vecs, docs=docs, meta=meta, setup=setup), f)\n",
    "    print(f\"Saved: {prefix}.faiss & {prefix}.pkl\")\n",
    "\n",
    "def load_index(prefix=save_prefix):\n",
    "    if not os.path.exists(f\"{prefix}.faiss\") or not os.path.exists(f\"{prefix}.pkl\"):\n",
    "        print(\"Saved files not found.\")\n",
    "        return\n",
    "    index = faiss.read_index(f\"{prefix}.faiss\")\n",
    "    with open(f\"{prefix}.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    globals()['_rag_index'] = index\n",
    "    globals()['_rag_vecs'] = data['vecs']\n",
    "    globals()['_rag_docs'] = data['docs']\n",
    "    globals()['_rag_meta'] = data['meta']\n",
    "    globals()['_rag_setup'] = data['setup']\n",
    "    print(\"Index loaded.\")\n",
    "\n",
    "# Buttons\n",
    "save_btn = widgets.Button(description='Save Index', icon='save')\n",
    "load_btn = widgets.Button(description='Load Index', icon='folder-open')\n",
    "out_persist = widgets.Output()\n",
    "\n",
    "def on_save(b):\n",
    "    out_persist.clear_output()\n",
    "    with out_persist:\n",
    "        save_index()\n",
    "\n",
    "def on_load(b):\n",
    "    out_persist.clear_output()\n",
    "    with out_persist:\n",
    "        load_index()\n",
    "\n",
    "save_btn.on_click(on_save)\n",
    "load_btn.on_click(on_load)\n",
    "\n",
    "display(widgets.HBox([save_btn, load_btn]), out_persist)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}